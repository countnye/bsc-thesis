{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook to use only one GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce008765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.moabb import MOABBDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, preprocess, Preprocessor)\n",
    "from numpy import multiply\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b315848",
   "metadata": {},
   "source": [
    "# Preprocessing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ef50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=None)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    low_cut_hz = 4.  # low cut frequency for filtering\n",
    "    high_cut_hz = 38.  # high cut frequency for filtering\n",
    "    # Parameters for exponential moving standardization\n",
    "    '''\n",
    "    CHECK IF THE FACTOR IS SAME AS 0.999 MENTIONED IN\n",
    "    THE ARTICLES\n",
    "    '''\n",
    "    factor_new = 1e-3\n",
    "    init_block_size = 1000\n",
    "    # Factor to convert from V to uV\n",
    "    factor = 1e6\n",
    "    iir_params = dict(order=3, ftype='butter', output='sos')\n",
    "\n",
    "    preprocessors = [\n",
    "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "        Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz, iir_params=iir_params, method='iir', phase='forward'),  # Third order butterworth filter\n",
    "        # The logs say it's a causal filter but the order is 6?\n",
    "        Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                    factor_new=factor_new, init_block_size=init_block_size)\n",
    "    ]\n",
    "\n",
    "    return preprocess(dataset, preprocessors)\n",
    "\n",
    "def epoch_data(dataset):\n",
    "    trial_start_offset_seconds = -0.5\n",
    "    # Extract sampling frequency, check that they are same in all datasets\n",
    "    sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "    # Calculate the trial start offset in samples.\n",
    "    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "    # Create windows using braindecode function for this. It needs parameters to\n",
    "    # define how trials should be used.\n",
    "    windows_dataset = create_windows_from_events(\n",
    "        dataset,\n",
    "        trial_start_offset_samples=trial_start_offset_samples,\n",
    "        trial_stop_offset_samples=0,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return windows_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects a BaseConcatDataset object\n",
    "# Iterate through subject datasets and create dataset of 9 rows and 576 columns\n",
    "# (9 subjects and 576 trials).\n",
    "def create_dataframe_helper(dataset):\n",
    "    subjects_lst = []\n",
    "    subjects_targets = []\n",
    "    for subject_id in range(0, len(dataset)):\n",
    "        # Append to list a set of inputs and targets from each run\n",
    "        # in subject dataset\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        subject_dataset = dataset[subject_id].datasets\n",
    "        for run in subject_dataset:\n",
    "            for trial in run:\n",
    "                inputs.append(trial[0])\n",
    "                targets.append(trial[1])\n",
    "        subjects_lst.append(inputs)\n",
    "        subjects_targets.append(targets)\n",
    "\n",
    "    return np.asarray(subjects_lst), np.asarray(subjects_targets)\n",
    "\n",
    "\n",
    "def create_dataframe(processed_data):\n",
    "    # Data to be saved gonna have shape (9, 576, 22, 1125)\n",
    "    # 9 subjects. 576 trials each. 22 channels. 1125 timestamps\n",
    "    split_data = processed_data.split('subject')\n",
    "    split_data = [split_data[str(i)] for i in range(1, 9 + 1)]\n",
    "    inputs, targets = create_dataframe_helper(split_data)\n",
    "    return inputs, targets\n",
    "\n",
    "def onehot(targets):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    targets = targets.reshape(-1,1)\n",
    "    targets = encoder.fit_transform(targets)\n",
    "    n_subj = 9\n",
    "    n_trials = 576\n",
    "    n_classes = 4\n",
    "    targets = targets.reshape(n_subj, n_trials, n_classes)\n",
    "    return targets\n",
    "    \n",
    "\n",
    "def get_x_y(inputs, targets):\n",
    "    n_subjects = inputs.shape[0]\n",
    "    n_runs = inputs.shape[1] * inputs.shape[0]\n",
    "    channels = inputs.shape[2]\n",
    "    timestamps = inputs.shape[3]\n",
    "    n_classes = targets.shape[2]\n",
    "    X = np.vstack(inputs).reshape(n_runs, channels, timestamps)\n",
    "    Y = np.vstack(targets).reshape(n_runs, n_classes)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d19d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = epoch_data(preprocess_data(load_dataset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f25e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = create_dataframe(processed_data)\n",
    "targets = onehot(targets)\n",
    "print(inputs.shape, targets.shape)\n",
    "save('all_subject_runs', inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90ceb0",
   "metadata": {},
   "source": [
    "# Lockbox Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_bachelors import *\n",
    "from file_functions import *\n",
    "from numpy import floor\n",
    "from numpy import random\n",
    "from sklearn.model_selection import KFold\n",
    "dataset = load('all_subject_runs')\n",
    "loaded_inputs = dataset['inputs']\n",
    "loaded_targets = dataset['targets']\n",
    "\n",
    "# Create Lockbox for each subject\n",
    "n_s = 9  # Number of subjects\n",
    "\n",
    "'''\n",
    "NEED TO IGNORE THE TEST SUBJECT AND LOXCKBOX THE REST!\n",
    "YOU CAN USE KFOLD SPLIT FOR THISb  \n",
    "'''\n",
    "\n",
    "kfold_lock = KFold(n_splits= n_s, shuffle= False)\n",
    "\n",
    "lockbox_input = []\n",
    "lockbox_target = []\n",
    "\n",
    "# Aside from the test subject, create a lockbox test set containing 10%\n",
    "# of each subject's data.\n",
    "for train_idx, test_idx in kfold_lock.split(loaded_inputs, loaded_targets):\n",
    "    subject_in = []\n",
    "    subject_tar = []\n",
    "    \n",
    "    for idx in train_idx:\n",
    "        subject_inputs = loaded_inputs[idx]\n",
    "        subject_targets = loaded_targets[idx]\n",
    "        num_trials= subject_inputs.shape[0]\n",
    "        # Get random 10% of subject's trials\n",
    "        idx = np.random.randint(num_trials, size=int(0.1 * num_trials))\n",
    "        subject_in.append(subject_inputs[idx])\n",
    "        subject_tar.append(subject_targets[idx])\n",
    "        \n",
    "    subject_in = np.vstack(np.array(subject_in))\n",
    "    subject_tar = np.vstack(np.array(subject_tar))\n",
    "    lockbox_input.append(subject_in)\n",
    "    lockbox_target.append(subject_tar)\n",
    "    \n",
    "\n",
    "lockbox_input = np.array(lockbox_input)\n",
    "lockbox_target = np.array(lockbox_target)\n",
    "\n",
    "# How to use lockbox: dim-0 is the Nth subject to test on (while model is trained on\n",
    "# N-1 subjects). \n",
    "save('lockbox_mcdropout', dict({'inputs': lockbox_input, 'targets': lockbox_target}))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
