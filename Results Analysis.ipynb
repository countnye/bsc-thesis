{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82b0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Set notebook to use only one GPU\n",
    "%env CUDA_VISIBLE_DEVICES=1     \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8eac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keras Uncertainty will use standalone Keras backend"
     ]
    }
   ],
   "source": [
    "from result_analysis_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa4ddf",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "Analyse results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d765c1",
   "metadata": {},
   "source": [
    "# Accuracy and average uncertainty:\n",
    "Data shape of preds and lockbox: (9, 50, 576, 4), 50 forward passes. \n",
    "\n",
    "'''\n",
    "preds and lockbox shape: (9, 50, 576, 4).\n",
    "Axis -3 (50) should be collapsed only in the \n",
    "following cases:\n",
    "    - accuracy\n",
    "    - std. dev.\n",
    "Collapsing of the axis for uncertainty is in\n",
    "models_bachelors.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d901a1",
   "metadata": {},
   "source": [
    "### Accuracy of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eacadb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n",
      "data shape: (9, 576, 4)\n",
      "data shape: (9, 576, 4)\n",
      "0 (576,)\n",
      "1 (576,)\n",
      "2 (576,)\n",
      "3 (576,)\n",
      "4 (576,)\n",
      "5 (576,)\n",
      "6 (576,)\n",
      "7 (576,)\n",
      "8 (576,)\n",
      "data shape: (9, 456, 4)\n",
      "data shape: (9, 456, 4)\n",
      "0 (456,)\n",
      "1 (456,)\n",
      "2 (456,)\n",
      "3 (456,)\n",
      "4 (456,)\n",
      "5 (456,)\n",
      "6 (456,)\n",
      "7 (456,)\n",
      "8 (456,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fa8d45522793>:39: RuntimeWarning: Mean of empty slice.\n",
      "  key = np.array(key).mean(axis=0)\n",
      "/home/pmanivannan/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-3-fa8d45522793>:42: RuntimeWarning: Mean of empty slice.\n",
      "  key = np.array(key).mean(axis=0)\n",
      "<ipython-input-3-fa8d45522793>:45: RuntimeWarning: Mean of empty slice.\n",
      "  key = np.array(key).mean(axis=0)\n",
      "<ipython-input-3-fa8d45522793>:48: RuntimeWarning: Mean of empty slice.\n",
      "  key = np.array(key).mean(axis=0)\n"
     ]
    }
   ],
   "source": [
    "acc_mcdropconnect = {'test': [], 'lockbox': []}\n",
    "acc_mcdropout = {'test': [], 'lockbox': []}\n",
    "acc_ensemble_dropout = {'test': [], 'lockbox': []}\n",
    "acc_duq = {'test': [], 'lockbox': []}\n",
    "acc_flipout = {'test': [], 'lockbox': []}\n",
    "NUM = 50  # Number of prediction sets\n",
    "# Load each prediction set and compute avg accuracy for each set, adding them to list\n",
    "for n in range(NUM):\n",
    "    methods = load_dict_from_hdf5(f'predictions/predictions_duq.h5')\n",
    "    for name, method in methods.items():\n",
    "        isStandard = True\n",
    "        # print(f'y_true: {method[\"test\"][\"labels\"].argmax(axis=-1).shape}, y_pred: {method[\"test\"][\"preds\"].argmax(axis=-1).shape}')\n",
    "        test_accs = get_accuracies(method[\"test\"], isStandard)   # Get test set accuracies\n",
    "        lock_accs = get_accuracies(method[\"lockbox\"], isStandard)\n",
    "        if name == 'mcdropconnect':\n",
    "            acc_mcdropconnect['test'].append(test_accs)\n",
    "            acc_mcdropconnect['lockbox'].append(lock_accs)\n",
    "        elif name == 'mcdropout':\n",
    "            acc_mcdropout['test'].append(test_accs)\n",
    "            acc_mcdropout['lockbox'].append(lock_accs)\n",
    "        elif name == 'standard':\n",
    "            acc_std['test'].append(test_accs)\n",
    "            acc_std['lockbox'].append(lock_accs)\n",
    "        elif name == 'standard_dropconnect':\n",
    "            acc_std_mcdropconnect['test'].append(test_accs)\n",
    "            acc_std_mcdropconnect['lockbox'].append(lock_accs)\n",
    "        elif name == 'flipout':\n",
    "            acc_flipout['test'].append(test_accs)\n",
    "            acc_flipout['lockbox'].append(lock_accs)\n",
    "        elif name == 'duq':\n",
    "            acc_duq['test'].append(test_accs)\n",
    "            acc_duq['lockbox'].append(lock_accs)\n",
    "        else:\n",
    "            acc_ensemble_dropout['test'].append(test_accs)\n",
    "            acc_ensemble_dropout['lockbox'].append(lock_accs)\n",
    "\n",
    "# Only for UQ:\n",
    "for name, key in acc_mcdropconnect.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_mcdropout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_ensemble_dropout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_flipout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duq\n",
      "test set avg acc: 55.421 +/- 9.163\n",
      "lockbox set avg acc: 70.468 +/- 2.929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import round\n",
    "\n",
    "r = 5\n",
    "# print(round(np.std(acc_mcdropconnect[\"test\"]), 5))\n",
    "# print('mcdropout')\n",
    "# print(acc_mcdropout)\n",
    "# print(f'test set avg acc: {round(np.mean(acc_mcdropout[\"test\"]), r) * 100} +/- {round(np.std(acc_mcdropout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_mcdropout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_mcdropout[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('mcdropconnect')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_mcdropconnect[\"test\"]), r) * 100} +/- {round(np.std(acc_mcdropconnect[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_mcdropconnect[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_mcdropconnect[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('standard_dropout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_std[\"test\"]), r) * 100} +/- {round(np.std(acc_std[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_std[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_std[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "# print('standard_dropconnect')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_std_mcdropconnect[\"test\"]), r) * 100} +/- {round(np.std(acc_std_mcdropconnect[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_std_mcdropconnect[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_std_mcdropconnect[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "# print('ensemble_dropout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_ensemble_dropout[\"test\"]), r) * 100} +/- {round(np.std(acc_ensemble_dropout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_ensemble_dropout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_ensemble_dropout[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "print('duq')\n",
    "print(f'test set avg acc: {round(np.mean(acc_duq[\"test\"]), r) * 100} +/- {round(np.std(acc_duq[\"test\"]), r) * 100}')\n",
    "print(f'lockbox set avg acc: {round(np.mean(acc_duq[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_duq[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('flipout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_flipout[\"test\"]), r) * 100} +/- {round(np.std(acc_flipout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_flipout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_flipout[\"lockbox\"]), r) * 100}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88f768",
   "metadata": {},
   "source": [
    "### Average normalised predictive entropy\n",
    "\n",
    "And associated normalised intersection for comparison [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pred_entropy_plots(dataset, method, unc_method):\n",
    "    bin_size = 0.05\n",
    "    entropy_correct = []\n",
    "    entropy_wrong = []\n",
    "    # For data loading standard predictions are in a single file\n",
    "    if 'standard' in method:\n",
    "        N = 1\n",
    "        isStandard = True\n",
    "    else:\n",
    "        N = 50\n",
    "        isStandard = False\n",
    "\n",
    "    # Iterate over all prediction sets.\n",
    "    if not isStandard:\n",
    "        for n in range(N):\n",
    "            methods = load_predictions(n, isStandard)\n",
    "            data = methods[method][dataset]\n",
    "            entropy = get_uncertainty(data,unc_method)\n",
    "            Y_true = data['labels']    # shape: (9,576,4)\n",
    "            corrects = get_corrects(Y_true, data['preds'], axis=-1) # Get corrects across ALL subjects\n",
    "            # Append the nth prediction's uncertainty estimations\n",
    "            entropy_correct.append(entropy[corrects])\n",
    "            entropy_wrong.append(entropy[~corrects])\n",
    "            # For distribution plots of predictive entropy\n",
    "    else:\n",
    "        methods = load_predictions(0, isStandard)\n",
    "        data = methods[method][dataset]\n",
    "        print()\n",
    "        entropy = get_uncertainty(data, unc_method)\n",
    "        Y_true = data['labels']    # shape: (9,576,4)\n",
    "        corrects = get_corrects(Y_true, data['preds'], axis=-1) # Get corrects across ALL subjects\n",
    "        # Append the nth prediction's uncertainty estimations\n",
    "        entropy_correct.append(entropy[corrects])\n",
    "        entropy_wrong.append(entropy[~corrects])\n",
    "\n",
    "    '''\n",
    "    Check for data mismatch: entropy_correct is probably a list of np arrays instead of \n",
    "    1 cohesive np array \n",
    "    '''\n",
    "    entropy_correct = np.hstack(entropy_correct)\n",
    "    entropy_wrong = np.hstack(entropy_wrong)\n",
    "    r = 5\n",
    "    unc_cor = np.mean(entropy_correct)\n",
    "    unc_cor_std = np.std(entropy_correct)\n",
    "    unc_in = np.mean(entropy_wrong)\n",
    "    unc_in_std = np.std(entropy_wrong)\n",
    "    print(f\"{dataset} avg. {unc_method} correct: {unc_cor:.5f} +/ {unc_cor_std:.5f}\")\n",
    "    print(f\"{dataset} avg. {unc_method} wrong: {unc_in:.5f} +/ {unc_in_std:.5f}\")\n",
    "\n",
    "    # hist_data = [entropy_correct, entropy_wrong]    \n",
    "    # group_labels = ['Correct', 'Incorrect']\n",
    "\n",
    "    # # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "    # hist_correct, bins_correct, _ = plt.hist(entropy_correct, bins=20, density=True, alpha=0.5, label='Correct')\n",
    "    # hist_wrong, bins_wrong, _ = plt.hist(entropy_wrong, bins=20, density=True, alpha=0.5, label='Wrong')\n",
    "    # plt.legend()\n",
    "    # # plt.show()\n",
    "\n",
    "    # # Calculate overlap using histogram intersection\n",
    "    # overlap = np.sum(np.minimum(hist_correct, hist_wrong))\n",
    "\n",
    "    # # Normalize overlap between 0 and 1\n",
    "    # normalized_overlap = overlap / np.sum(hist_correct)\n",
    "\n",
    "    # print(\"Overlap:\", normalized_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['mcdropconnect', 'mcdropout', 'standard', 'standard_dropconnect']\n",
    "\n",
    "for method in methods:\n",
    "    print(f'{method}\\n')\n",
    "    # False makes calculations off of mutual information\n",
    "    avg_pred_entropy_plots('test', method, 'shannon-entropy')\n",
    "    avg_pred_entropy_plots('lockbox', method, 'shannon-entropy')\n",
    "    # avg_pred_entropy_plots('lockbox', method, 'mutual-information')\n",
    "    # avg_pred_entropy_plots('lockbox', method, 'shannon-entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7dbf67",
   "metadata": {},
   "source": [
    "### PLOT ACC COV ON X AND Y AXIS, NOT THRESHOLD\n",
    "These plots are to select a threshold for treating uncertainty as a binary classification task. Once the threshold is selected from the plot, you can compute the uncertainty accuracy, precision, sensitivity, specificity ROC plots and AUROC.\n",
    "\n",
    "So accuracy and coverage will be computed for predictions that are above a certain uncertainty threshold\n",
    "\n",
    "Explanations for very poor entropy based accuracy-coverage plots:\n",
    "Because of high inter-subject variability, the specific model is unable to fully learn subject-independent features, leading to a high degree of epistemic uncertainty (my theory). Maybe I can confirm this by disentangling uncertainty... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3233a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "makes acc cov plot for a single method\n",
    "isEntropy is a bool to set whether to use entropy threshold\n",
    "or softmax prob threshold. Key is the dataset name ('test' or 'lockbox').\n",
    "method is the name of the method.\n",
    "'''\n",
    "def make_acc_cov_plot(method, key, isEntropy):\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    N = 50\n",
    "    accs = []\n",
    "    coverages = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for n in range(N):\n",
    "        methods = load_dict_from_hdf5(f'predictions/predictions_{n}.h5')\n",
    "        data = methods[method][key]\n",
    "        y_pred.append(np.vstack(data['preds']))\n",
    "        y_true.append(np.vstack(data['labels']))\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "\n",
    "    if isEntropy:\n",
    "        unc = predictive_uncertainty(y_pred)\n",
    "    for t in thresholds:\n",
    "        if isEntropy:\n",
    "            accepted_idx = (unc < t)           # Accept samples when uncertainty is BELOW threshold\n",
    "        else:\n",
    "            accepted_idx = y_pred.max(axis=1) > t           # Accept when predicted prob is ABOVE threshold\n",
    "        coverages.append(sum(accepted_idx) / y_pred.shape[0])                           # Coverage: How many samples rejected\n",
    "        acc = accuracy_score(y_pred=y_pred.argmax(axis=1)[accepted_idx], y_true=y_true.argmax(axis=1)[accepted_idx])  # Accuracy of accepted samples\n",
    "        accs.append(acc)\n",
    "\n",
    "    if isEntropy:\n",
    "        coverages = np.flip(np.array(coverages))  # Flip for entropy threshold\n",
    "        accs = np.flip(np.array(accs))\n",
    "    else:\n",
    "        coverages = np.array(coverages)\n",
    "        accs = np.array(accs)\n",
    "\n",
    "    plt.plot(coverages, accs, color='red')\n",
    "\n",
    "    plt.title('Coverage and Accuracy vs Threshold')\n",
    "    plt.legend()\n",
    "    # plt.savefig('threshold_plots.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Finding equivalent threshold value\n",
    "    point = np.hstack(np.argwhere(accs > coverages))\n",
    "    print(f'threshold value for {method}: {point[0] * 0.001}\\n acc at threshold: {accs[point[0]]}\\n coverage at threshold: {coverages[point[0]]}')\n",
    "\n",
    "\n",
    "\n",
    "methods = ['mcdropconnect', 'mcdropout', 'standard']\n",
    "\n",
    "for method in methods:\n",
    "    make_acc_cov_plot(method, 'test', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc\n",
    "import matplotlib.patches as mpatches\n",
    "from numpy import round\n",
    "\n",
    "'''\n",
    "Gets a numpy array down to a 2D array\n",
    "'''\n",
    "def get_in_shape(data):\n",
    "    # If data has shape of 2 elements or less, assume it's already in shape\n",
    "    if len(data.shape) < 3:\n",
    "        return data\n",
    "    else:\n",
    "        while len(data.shape) > 2:\n",
    "            data = np.vstack(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "'''\n",
    "data can be whatever shape.\n",
    "manual roc plot creation reqs:\n",
    "    y_pred: (9*50, 50, 576, 4)\n",
    "'''\n",
    "def make_roc_plot(y_true, y_pred, isStandard, unc_method):\n",
    "    '''\n",
    "    y_pred can be either of shape (50, 9, 50, 576, 4) or  (9, 50, 576, 4). We need it in shape (X, 4).\n",
    "    y_true can be either of shape (50, 9, 576, 4) or (9, 576, 4).\n",
    "    So apply same algorithm to get these sets into the shape (X, 4)\n",
    "    '''\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}')\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    unc = get_uncertainty(y_pred, unc_method).flatten()\n",
    "    y_true = get_in_shape(y_true)\n",
    "    y_pred = get_in_shape(y_pred) if isStandard else get_in_shape(y_pred.mean(axis=-3))\n",
    "    \n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}, certains: {unc.shape}')\n",
    "    for t in thresholds:\n",
    "        '''\n",
    "        Order is reversed because FPR and TPR are reversed for some reason\n",
    "        '''\n",
    "        certains = (t < unc)           # Certain when uncertainty is below threshold\n",
    "        uncertains = (t > unc)\n",
    "        # Calculate TPR and FPR\n",
    "        tp = sum(y_pred.argmax(axis=1)[uncertains] != y_true.argmax(axis=1)[uncertains])     # N. preds uncertain predictions that are incorrect\n",
    "        fn = sum(y_pred.argmax(axis=1)[certains] != y_true.argmax(axis=1)[certains])      # Prediction that's certain and incorrect\n",
    "        fp = sum(y_pred.argmax(axis=1)[uncertains] == y_true.argmax(axis=1)[uncertains])   # prediction that's uncertain and correct\n",
    "        tn = sum(y_pred.argmax(axis=1)[certains] == y_true.argmax(axis=1)[certains]) # Prediction that's certain and correct\n",
    "        # print(f'tp: {tp} fn: {fn} fp: {fp} tn: {tn}')\n",
    "        fpr.append(fp / (fp + tn))\n",
    "        tpr.append(tp / (tp + fn))\n",
    "    # fig1, ax1 = plt.subplots()\n",
    "    # hist_correct, bins_correct, _ = ax1.hist(auc, bins=10, density=False, alpha=0.5, label='Correct')\n",
    "    # fig1.show()\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "\n",
    "'''\n",
    "I calculate AUROC and plot ROC separately because I want to get\n",
    "mean AUROC of all 50 prediction sets along with their variance.\n",
    "Then I plot ROC with all 50 prediction sets.\n",
    "'''\n",
    "def roc_plot_and_auroc(method, key, unc_method):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    aucs_lst = []\n",
    "    # isStandard = True if 'standard' in method else False\n",
    "    isStandard = False\n",
    "    # num_predictions = 50 if not isStandard else 1\n",
    "    num_predictions = 1\n",
    "    # creation of set of 50 predictions, as well as AUROC score calculation\n",
    "    for n in range(num_predictions):\n",
    "        # methods = load_predictions(n, 'duq')\n",
    "        methods = load_dict_from_hdf5(f'predictions/predictions_ensemble_dropout.h5')\n",
    "        data = methods[method][key]\n",
    "        tpr, fpr = make_roc_plot(data['labels'], data['preds'], isStandard, unc_method)\n",
    "        # print(f'y_true shape: {y_true_roc.shape} y_pred: {y_pred_roc.shape}')\n",
    "        auroc_score = auc(tpr, fpr)\n",
    "        aucs_lst.append(auroc_score)\n",
    "        y_pred.append(data['preds'])\n",
    "        y_true.append(data['labels'])\n",
    "\n",
    "    tpr, fpr = make_roc_plot(np.vstack(y_true), np.vstack(y_pred), isStandard, unc_method)\n",
    "\n",
    "    return tpr, fpr, aucs_lst\n",
    "\n",
    "\n",
    "# aucs_test = {'predictive-entropy':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]},\n",
    "#           'shannon-entropy':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]},\n",
    "#           'mutual-information':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}\n",
    "#         }\n",
    "# aucs_test = {'predictive-entropy': {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}}\n",
    "# methods = {'mcdropconnect':'C0', 'mcdropout':'C1', 'standard':'C4', 'standard_dropconnect':'C9'}\n",
    "# aucs_test = {'predictive-entropy': {'ensemble_dropout': []}}\n",
    "# aucs_test = {'predictive-entropy': {'duq': []}}\n",
    "aucs_test = {'predictive-entropy': {'ensemble_dropout': []}, 'shannon-entropy': {'ensemble_dropout': []}, 'mutual-information': {'ensemble_dropout': []}}\n",
    "# fig1, ax1 = plt.subplots(2, squeeze=False, figsize=(10, 20))\n",
    "# ax1 = ax1.flatten()\n",
    "key = \"test\"\n",
    "print(key)\n",
    "for unc_name, methods_dict in aucs_test.items():\n",
    "    print(unc_name)\n",
    "    for method, auc_lst in methods_dict.items():\n",
    "        tpr, fpr, ret_aucs = roc_plot_and_auroc(method, key, unc_name)\n",
    "        print(f'{key} AUC: {round(np.mean(ret_aucs), r) * 100} +/- {round(np.std(ret_aucs), r) * 100}')\n",
    "        # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "        print(method)\n",
    "        r = 6\n",
    "        # ax1[0].plot(tpr, fpr, color=methods[method])\n",
    "\n",
    "key = \"lockbox\"\n",
    "print(key)\n",
    "for unc_name, methods_dict in aucs_test.items():\n",
    "    print(unc_name)\n",
    "    for method, auc_lst in methods_dict.items():\n",
    "        r = 6\n",
    "        tpr, fpr, ret_aucs = roc_plot_and_auroc(method, key, unc_name)\n",
    "        print(f'{key} AUC: {round(np.mean(ret_aucs), r) * 100} +/- {round(np.std(ret_aucs), r) * 100}')\n",
    "        # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "        print(method)\n",
    "        # ax1[1].plot(tpr, fpr, color=methods[method])\n",
    "\n",
    "# red_patch = mpatches.Patch(color=methods['mcdropconnect'], label='MC-DropConnect')\n",
    "# green_patch = mpatches.Patch(color=methods['mcdropout'], label='MC-Dropout')\n",
    "# blue_patch = mpatches.Patch(color=methods['standard'], label='Standard Dropout')\n",
    "# yellow_patch = mpatches.Patch(color=methods['standard_dropconnect'], label='Standard Dropconnect')\n",
    "\n",
    "# ax1[0].legend(handles=[red_patch, green_patch, blue_patch, yellow_patch], fontsize=20)\n",
    "# ax1[0].set_title(f'Out-of-population ROC', fontsize=35)\n",
    "# ax1[0].set_xlabel(\"FPR\", fontsize=30)\n",
    "# ax1[0].set_ylabel(\"TPR\", fontsize=30)\n",
    "\n",
    "# ax1[1].legend(handles=[red_patch, green_patch, blue_patch, yellow_patch], fontsize=20)\n",
    "# ax1[1].set_title(f'Within-population ROC', fontsize=35)\n",
    "# ax1[1].set_xlabel(\"FPR\", fontsize=30)\n",
    "# ax1[1].set_ylabel(\"TPR\", fontsize=30)\n",
    "\n",
    "# # plt.rcParams.update({'font.size': 22})\n",
    "# # fig1.savefig(f'roc_plot_pred_entropy_w_10_h_20.pdf')\n",
    "# ax1[0].tick_params(axis='x', labelsize=20)\n",
    "# ax1[0].tick_params(axis='y', labelsize=20)\n",
    "# ax1[1].tick_params(axis='x', labelsize=20)\n",
    "# ax1[1].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# fig1.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88310a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig(f'roc_plot_pred_entropy_w_10_h_20_large.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is a significant difference between the AUROCs of two methods with a certain metric on one set, need to construct and save a MATLAB struct with two elements:\n",
    "\n",
    "- spsizes: 2 x 1 vector with samples sizes for X and Y, uncertainties and targets\n",
    "- ratings: K x N matrix. K is uncertainties/targets of each model, it is the row. N is the sum of len(X) and len(Y) and first len(X) elements are uncertainties and last len(Y) elements are targets\n",
    "\n",
    "Then I gotta save this data as a MATLAB struct while in python somehow\n",
    "\n",
    "Things I want to compare for test and lockbox:\n",
    "- MC-Dropout: mutual information and shannon entropy\n",
    "- MC-DropConnect: mutual information and shannon entropy\n",
    "\n",
    "So 2 separate files to save, one for test and the other for lockbox\n",
    "\n",
    "y_true/ground truth labels will be 0, 1. And will be 1 when y_true != y_pred and 0 when it isn't. So basically, 1 for all incorrects (which should be uncertains) and 0 for all corrects (which should be certains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compare_auc_delong_xu as com\n",
    "\n",
    "'''\n",
    "Code for saving as a MATLAB struct\n",
    "'''\n",
    "\n",
    "'''\n",
    "Gets a numpy array down to a 2D array\n",
    "'''\n",
    "def get_in_shape(data):\n",
    "    # If data has shape of 2 elements or less, assume it's already in shape\n",
    "    if len(data.shape) < 3:\n",
    "        return data\n",
    "    else:\n",
    "        while len(data.shape) > 2:\n",
    "            data = np.vstack(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "'''\n",
    "Returns unc, y_pred\n",
    "'''\n",
    "def get_ratings_helper(y_true, y_pred, isStandard, unc_method):\n",
    "    '''\n",
    "    y_pred can be either of shape (50, 9, 50, 576, 4) or  (9, 50, 576, 4). We need it in shape (X, 4).\n",
    "    y_true can be either of shape (50, 9, 576, 4) or (9, 576, 4).\n",
    "    So apply same algorithm to get these sets into the shape (X, 4)\n",
    "    '''\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    unc = get_uncertainty(y_pred, unc_method).flatten()\n",
    "    y_pred = get_in_shape(y_pred) if isStandard else get_in_shape(y_pred.mean(axis=-3))\n",
    "    y_true = (get_in_shape(y_true).argmax(axis=1) != y_pred.argmax(axis=1)).astype(int)   # y_true is array of 0s and 1s where 1 is predictions that are incorrect\n",
    "\n",
    "    return unc, y_true\n",
    "\n",
    "def get_ratings(method, key, unc_method):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    isStandard = True if 'standard' in method else False\n",
    "    num_predictions = 50 if not isStandard else 1\n",
    "    # creation of set of 50 predictions, as well as AUROC score calculation\n",
    "    for n in range(num_predictions):\n",
    "        methods = load_predictions(n, isStandard)\n",
    "        data = methods[method][key]\n",
    "        y_pred.append(data['preds'])\n",
    "        y_true.append(data['labels'])\n",
    "\n",
    "    unc, targets = get_ratings_helper(np.vstack(y_true), np.vstack(y_pred), isStandard, unc_method)\n",
    "\n",
    "    return unc, targets\n",
    "    \n",
    "\n",
    "aucs_test = {'shannon-entropy':\n",
    "          {'mcdropconnect':[], 'mcdropout':[], 'standard': [], 'standard_dropconnect': []},\n",
    "          'mutual-information':\n",
    "          {'mcdropconnect':[], 'mcdropout':[]},\n",
    "          'predictive-entropy':\n",
    "          {'mcdropconnect':[], 'mcdropout':[], 'standard': [], 'standard_dropconnect': []}\n",
    "        }\n",
    "key = \"test\"\n",
    "print(key)\n",
    "for unc_name, methods_dict in aucs_test.items():\n",
    "    print(unc_name)\n",
    "    for method, auc_lst in methods_dict.items():\n",
    "        unc, targets = get_ratings(method, key, unc_name)\n",
    "        print(method)\n",
    "        methods_dict[method] = {'unc': unc, 'targets': targets}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = aucs_test['shannon-entropy']['mcdropout']['targets']\n",
    "z, value = com.delong_roc_test(ground_truth, aucs_test['shannon-entropy']['mcdropout']['unc'], aucs_test['mutual-information']['mcdropout']['unc'])\n",
    "print(f'p-value for null-hypothesis that there is no difference between shannon entropy and mutual information AUROC for MC-Dropout: Z: {z}, p: {np.power(10, value)}')\n",
    "\n",
    "\n",
    "ground_truth = aucs_test['shannon-entropy']['mcdropconnect']['targets']\n",
    "z, value = com.delong_roc_test(ground_truth, aucs_test['shannon-entropy']['mcdropconnect']['unc'], aucs_test['mutual-information']['mcdropconnect']['unc'])\n",
    "print(f'p-value for null-hypothesis that there is no difference between shannon entropy and mutual information AUROC for MC-DropConnect: Z: {z}, p: {np.power(10, value)}')\n",
    "\n",
    "ground_truth = aucs_test['predictive-entropy']['mcdropconnect']['targets']\n",
    "z, value = com.delong_roc_test(ground_truth, aucs_test['predictive-entropy']['mcdropconnect']['unc'], aucs_test['shannon-entropy']['mcdropconnect']['unc'])\n",
    "print(f'p-value for null-hypothesis that there is no difference between standard and mcdropout using predictive entropy: Z: {z}, p: {np.power(10, value)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "'''\n",
    "I want to compute if there is a significant difference between: MC-Dropconnect shannon entropy and mutual information\n",
    "and MC-Dropout shannon entropy and mutual information.\n",
    "And MC-Dropconnect predictive entropy and Mc-Dropout predictive entropy (if mc dropconnect better than mcdropout, then its better than the others)\n",
    "'''\n",
    "\n",
    "# print(stats.mannwhitneyu(aucs_test['shannon-entropy']['m']['unc'], aucs_test['mutual-information']['mcdropout']['unc'], alternative='two-sided'))\n",
    "print(stats.ttest_ind(aucs_test['shannon-entropy']['mcdropout']['unc'], aucs_test['predictive-entropy']['mcdropout']['unc'], alternative='two-sided'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per subject scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per-subject uncertainties and AUROC\n",
    "This is exactly what I need:\n",
    "    - Per subject AUROC. This can only be done with array of shape (9, 576).\n",
    "        - start w/ (50, 9, 50, 576, 4) for a method.\n",
    "        - Mean axis=0 -> (9, 50, 576, 4)\n",
    "        - Get uncertainties -> (9, 576) -> Mean axis -1=Avg. uncertainties -> (9, 1)\n",
    "        - For each subject in axis 0, calculate AUROC to get final array of (9, 1)\n",
    "    - Array of shape (9, 1) for uncertanties\n",
    "        - Get uncertainties -> (9, 576) -> Mean axis -1=Avg. uncertainties -> (9, 1)\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats as stats\n",
    "from numpy import round\n",
    "\n",
    "def load_all_predictions():\n",
    "    predictions = {'mcdropout': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'mcdropconnect': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'standard': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'standard_dropconnect': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}}\n",
    "            }\n",
    "    # Load all UQ method predictions\n",
    "    N = 50\n",
    "    for n in range(N):\n",
    "        dataset = load_predictions(n, False)\n",
    "        # No need to append the labels because they're the same each time.\n",
    "        # So only need to append them once.\n",
    "        predictions['mcdropout']['test']['preds'].append(dataset['mcdropout']['test']['preds'])\n",
    "        predictions['mcdropout']['lockbox']['preds'].append(dataset['mcdropout']['lockbox']['preds'])\n",
    "        predictions['mcdropconnect']['test']['preds'].append(dataset['mcdropconnect']['test']['preds'])\n",
    "        predictions['mcdropconnect']['lockbox']['preds'].append(dataset['mcdropconnect']['lockbox']['preds'])\n",
    "\n",
    "    # Load all Standard method predictions\n",
    "    dataset = load_predictions(1, True)\n",
    "    predictions['standard']['test']['preds'].append(dataset['standard']['test']['preds'])\n",
    "    predictions['standard']['lockbox']['preds'].append(dataset['standard']['lockbox']['preds'])\n",
    "    predictions['standard_dropconnect']['test']['preds'].append(dataset['standard_dropconnect']['test']['preds'])\n",
    "    predictions['standard_dropconnect']['lockbox']['preds'].append(dataset['standard_dropconnect']['lockbox']['preds'])\n",
    "\n",
    "    # Load all the labels once into their respective method dicts\n",
    "    for name, method in predictions.items():\n",
    "        # standard labels are same as UQ method labels.\n",
    "        method['test']['labels'] = dataset['standard']['test']['labels']\n",
    "        method['lockbox']['labels'] = dataset['standard']['lockbox']['labels']\n",
    "        # convert relevant lists to numpy arrays\n",
    "        method['test']['preds'] = np.array(method['test']['preds'])\n",
    "        method['lockbox']['preds'] = np.array(method['lockbox']['preds'])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "'''\n",
    "data can be whatever shape.\n",
    "manual roc plot creation reqs:\n",
    "    y_pred: (50, 576, 4)\n",
    "'''\n",
    "def get_fpr_tpr(y_true, y_pred, unc, isStandard):\n",
    "    '''\n",
    "    y_pred can be either of shape (50, 9, 50, 576, 4) or  (9, 50, 576, 4). We need it in shape (X, 4).\n",
    "    y_true can be either of shape (50, 9, 576, 4) or (9, 576, 4).\n",
    "    So apply same algorithm to get these sets into the shape (X, 4)\n",
    "    '''\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    y_pred = y_pred if isStandard else y_pred.mean(axis=-3)\n",
    "    \n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}, certains: {unc.shape}')\n",
    "    for t in thresholds:\n",
    "        '''\n",
    "        Order is reversed because FPR and TPR are reversed for some reason\n",
    "        '''\n",
    "        certains = (t < unc)           # Certain when uncertainty is below threshold\n",
    "        uncertains = (t > unc)\n",
    "        # Calculate TPR and FPR\n",
    "        tp = sum(y_pred.argmax(axis=1)[uncertains] != y_true.argmax(axis=1)[uncertains])     # N. preds uncertain predictions that are incorrect\n",
    "        fn = sum(y_pred.argmax(axis=1)[certains] != y_true.argmax(axis=1)[certains])      # Prediction that's certain and incorrect\n",
    "        fp = sum(y_pred.argmax(axis=1)[uncertains] == y_true.argmax(axis=1)[uncertains])   # prediction that's uncertain and correct\n",
    "        tn = sum(y_pred.argmax(axis=1)[certains] == y_true.argmax(axis=1)[certains]) # Prediction that's certain and correct\n",
    "        # print(f'tp: {tp} fn: {fn} fp: {fp} tn: {tn}')\n",
    "        fpr.append(fp / (fp + tn))\n",
    "        tpr.append(tp / (tp + fn))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "def get_auroc(y_true, y_pred, unc, isStandard):\n",
    "    tpr, fpr = get_fpr_tpr(y_true, y_pred, unc, isStandard)\n",
    "    return auc(tpr, fpr)\n",
    "\n",
    "def per_subject_metrics(data, isStandard):\n",
    "    key_set = data[key]        # Whether lockbox or preds of the method\n",
    "    y_true = key_set['labels']\n",
    "    y_preds = key_set['preds'].mean(axis=0)     # Take avg of 50 pred. sets if UQ preds\n",
    "    unc = predictive_uncertainty(y_preds, 'predictive-entropy')\n",
    "    per_subject_aucs = []\n",
    "    # print(y_preds.shape)\n",
    "    for subject_id in range(y_preds.shape[0]):\n",
    "        per_subject_aucs.append(get_auroc(y_true[subject_id], y_preds[subject_id], unc[subject_id], isStandard))\n",
    "\n",
    "    \n",
    "    return np.array(per_subject_aucs), unc.mean(axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "data: (50, 9, 50, 576, 4)\n",
    "method: 'mcdropconnect'/'mcdropout'/'standard'/'standard_dropconnect'\n",
    "key: 'test'/'lockbox'\n",
    "'''\n",
    "def do_everything(data, method, key):\n",
    "    # data shape for UQ preds: (50, 9, 50, 576, 4)\n",
    "    aucs_lst = []\n",
    "    isStandard = True if 'standard' in method else False\n",
    "    aurocs, uncertainties = per_subject_metrics(data, isStandard)\n",
    "    return aurocs, uncertainties\n",
    "\n",
    "\n",
    "aucs = {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}\n",
    "uncertainties = {'mcdropconnect':[], 'mcdropout':[], 'std':[], 'standard_dropconnect':[]}\n",
    "methods = {'mcdropconnect':'red', 'mcdropout':'green', 'standard':'blue', 'standard_dropconnect':'yellow'}\n",
    "fig1, ax1 = plt.subplots()\n",
    "key = \"test\"\n",
    "data = load_all_predictions()       # all prediction sets across all methods into a single object\n",
    "for method, values in data.items():\n",
    "    aurocs, uncs = do_everything(values, method, key)\n",
    "    # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "    print(method)\n",
    "    r = 6\n",
    "    aucs[method] = aurocs\n",
    "    uncertainties[method] = uncs\n",
    "    # print(f'{key} set avg AUROC: {np.mean(aucs)} +/- {np.std(aucs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, values in aucs.items():\n",
    "    print(f'\\n\\n{method}\\n aucs:{values}\\n uncs: {uncertainties[method]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For some reason, sklearn's auroc score was consistently ouputting 0.52-0.54 while the manually calculate ROC plots showed much \n",
    "greater than 0.5. Problem started when I started incorporating forward passes into the predictions and changing the implementation\n",
    "of uncertainty metrics. I fucking give up trying to fix it, fuck this stupid bullshit.\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats as stats\n",
    "from numpy import round\n",
    "\n",
    "'''\n",
    "I calculate AUROC and plot ROC separately because I want to get\n",
    "mean AUROC of all 50 prediction sets along with their variance.\n",
    "Then I plot ROC with all 50 prediction sets.\n",
    "'''\n",
    "\n",
    "\n",
    "    # y_true = values['lockbox']['labels']\n",
    "    # y_pred = predictive_uncertainty(values['lockbox']['preds']).flatten()\n",
    "    # # Iterate through entropies and create a new np array of entropies\n",
    "    # # (9*576, 2)\n",
    "\n",
    "    # # Gets corrects across all subjects as ints. 1: incorrect 0: correct\n",
    "    # y_true_roc = (~get_corrects(y_true, values['lockbox']['preds'], axis=-1)).astype(int).flatten()\n",
    "\n",
    "    # fpr, tpr, _ = metrics.roc_curve(y_true_roc,  y_pred)\n",
    "    # auc = metrics.roc_auc_score(y_true_roc, y_pred)\n",
    "def roc_plot(method, key):\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    fpr = []        # (N. uncertain predictions that are correct) / (N. certain predictions)\n",
    "    tpr = []        # (N. uncertain predictions that are incorrect) / (N. )\n",
    "    aucs_lst = []\n",
    "    # creation of set of 50 predictions, as well as AUROC score calculation\n",
    "    for n in range(N):\n",
    "        methods = load_predictions(n, False)\n",
    "        data = methods[method][key]\n",
    "        y_pred_roc = predictive_uncertainty(data['preds'], 'shannon-entropy').flatten()\n",
    "        y_true_roc = (~get_corrects(data['labels'], data['preds'], axis=-1)).astype(int).flatten()\n",
    "        print(f'y_true shape: {y_true_roc.shape} y_pred: {y_pred_roc.shape}')\n",
    "        aucs_lst.append(roc_auc_score(y_true_roc, y_pred_roc))\n",
    "        y_pred.append(data['preds'])\n",
    "        y_true.append(data['labels'])\n",
    "\n",
    "    '''\n",
    "    For manual ROC plot creation to FUCKING work, these are the requirements:\n",
    "        y_pred shape: (50, 9, 50, 576, 4). Then calculate uncertainty so shape becomes (50, 9, 576, 4). Then stack so it becomes (50*9*576, 4)\n",
    "        y_true shape: (50, 9, 576, 4). Stack so it becomes (50*9*576, 4).\n",
    "    So for creation of an ROC plot involving ALL the predictions, you have to stack twice.\n",
    "    But for ROC plot of a single prediction set, only gotta stack once...\n",
    "    So maybe keep stacking until shape ends up having only 2 elements.\n",
    "    '''\n",
    "    y_true = np.vstack(np.vstack(y_true))\n",
    "    y_pred = np.array(y_pred)\n",
    "    unc = predictive_uncertainty(y_pred, 'shanon-entropy').flatten()\n",
    "    y_pred = np.vstack(np.vstack(y_pred.mean(axis=-3)))\n",
    "    for t in thresholds:\n",
    "        certains = (t > unc)           # Certain when uncertainty is below threshold\n",
    "        uncertains = (t < unc)\n",
    "        # Calculate TPR and FPR\n",
    "        tp = sum(y_pred.argmax(axis=1)[uncertains] != y_true.argmax(axis=1)[uncertains])     # N. preds uncertain predictions that are incorrect\n",
    "        fn = sum(y_pred.argmax(axis=1)[certains] != y_true.argmax(axis=1)[certains])      # Prediction that's certain and incorrect\n",
    "        fp = sum(y_pred.argmax(axis=1)[uncertains] == y_true.argmax(axis=1)[uncertains])   # prediction that's uncertain and correct\n",
    "        tn = sum(y_pred.argmax(axis=1)[certains] == y_true.argmax(axis=1)[certains]) # Prediction that's certain and correct\n",
    "        tpr.append(tp / (tp + fn))\n",
    "        fpr.append(fp / (fp + tn))\n",
    "    # fig1, ax1 = plt.subplots()\n",
    "    # hist_correct, bins_correct, _ = ax1.hist(auc, bins=10, density=False, alpha=0.5, label='Correct')\n",
    "    # fig1.show()\n",
    "\n",
    "    return fpr, tpr, aucs_lst\n",
    "    # # plt.savefig('threshold_plots.png')\n",
    "\n",
    "\n",
    "aucs = {'mcdropconnect':[], 'mcdropout':[], 'std':[], 'standard_dropconnect':[]}\n",
    "methods = {'mcdropconnect':'red', 'mcdropout':'green', 'standard':'blue', 'standard_dropconnect':'yellow'}\n",
    "fig1, ax1 = plt.subplots()\n",
    "for method, color in methods.items():\n",
    "    key = \"test\"\n",
    "    fpr, tpr, auc_score = roc_plot(method, key)\n",
    "    # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "    print(method)\n",
    "    r = 6\n",
    "    aucs[method] = auc_score\n",
    "    print(f'{key} set avg AUROC: {np.mean(auc_score)} +/- {np.std(auc_score)}')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.plot(fpr, tpr, color=color)\n",
    "\n",
    "\n",
    "# print(stats.mannwhitneyu(aucs['mcdropout'], aucs['mcdropconnect'], alternative='two-sided'))\n",
    "# print(stats.mannwhitneyu(aucs['standard'], aucs['mcdropconnect'], alternative='two-sided'))\n",
    "# red_patch = mpatches.Patch(color='red', label='MC-DropConnect')\n",
    "# green_patch = mpatches.Patch(color='green', label='MC-Dropout')\n",
    "# blue_patch = mpatches.Patch(color='blue', label='Standard Dropout')\n",
    "# yellow_patch = mpatches.Patch(color='yellow', label='Standard Dropconnect')\n",
    "# ax1.legend(handles=[red_patch, green_patch, blue_patch, yellow_patch])\n",
    "# ax1.set_title('Test ROC')\n",
    "# ax1.set_xlabel(\"FPR\")\n",
    "# ax1.set_ylabel(\"TPR\")\n",
    "# fig1.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
